{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f9b9410e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to path: /Users/elshaday/DEV/10Academy/sentiment-analysis-week1\n"
     ]
    }
   ],
   "source": [
    "# Fix imports when running from notebooks/ folder\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Added to path: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bff8afb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/elshaday/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import nltk\n",
    "nltk.downloader.download(\"vader_lexicon\")\n",
    "\n",
    "\n",
    "from scripts import calculate_sentiment\n",
    "from src.data import DataManager\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "66ba8ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load cleaned data:\n",
      "Loading news data from ../data/processed_data/processed_news.csv\n",
      "News data loaded successfully.\n",
      "Loading stock data for AAPL from ../data/processed_data/processed_AAPL.csv\n",
      "Stock data for AAPL loaded successfully.\n",
      "Loading stock data for AMZN from ../data/processed_data/processed_AMZN.csv\n",
      "Stock data for AMZN loaded successfully.\n",
      "Loading stock data for GOOG from ../data/processed_data/processed_GOOG.csv\n",
      "Stock data for GOOG loaded successfully.\n",
      "Loading stock data for META from ../data/processed_data/processed_META.csv\n",
      "Stock data for META loaded successfully.\n",
      "Loading stock data for MSFT from ../data/processed_data/processed_MSFT.csv\n",
      "Stock data for MSFT loaded successfully.\n",
      "Loading stock data for NVDA from ../data/processed_data/processed_NVDA.csv\n",
      "Stock data for NVDA loaded successfully.\n",
      "All stock data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Load cleaned data:\")\n",
    "dm = DataManager()\n",
    "\n",
    "try:\n",
    "    news_df = dm.load_news_data(load_processed=True)\n",
    "    stock_data = dm.load_all_stocks(load_processed=True)\n",
    "except Exception as e:\n",
    "    print(f\"Data Loading Failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ad0d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis using compund score:\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentiment Analysis using compund score\")\n",
    "\n",
    "# Remove empty headlines\n",
    "sentiment_ready_news_df = news_df.dropna(subset=[\"headline_cleaned\"]).copy()\n",
    "\n",
    "# Calculate sentiment on cleaned headlines with no NA values\n",
    "sentiment_ready_news_df[\"sentiment\"] = calculate_sentiment(sentiment_ready_news_df[\"headline_cleaned\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "819760c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group news records by their published date and take the average sentiment for each day:\n",
      "                           avg_sentiment\n",
      "date_clean                              \n",
      "2009-02-14 00:00:00+00:00        0.22630\n",
      "2009-04-27 00:00:00+00:00        0.00000\n",
      "2009-04-29 00:00:00+00:00        0.00000\n",
      "2009-05-22 00:00:00+00:00        0.00000\n",
      "2009-05-27 00:00:00+00:00        0.79515\n"
     ]
    }
   ],
   "source": [
    "print(\"Group news records by their published date and take the average sentiment for each day:\")\n",
    "\n",
    "daily_sentiment = (\n",
    "    sentiment_ready_news_df\n",
    "    .groupby([\"date_clean\"])['sentiment']\n",
    "    .mean()\n",
    "    .to_frame()\n",
    "    .rename(columns={\"sentiment\": \"avg_sentiment\"})\n",
    ")\n",
    "\n",
    "print(daily_sentiment.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
