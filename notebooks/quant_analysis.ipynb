{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e57b6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e2dae09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stock data from CSV files\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading stock data from CSV files\")\n",
    "tickers = [\"AAPL\", \"AMZN\", \"GOOG\", \"META\", \"MSFT\", \"NVDA\"]\n",
    "\n",
    "stock_data = {}\n",
    "\n",
    "for t in tickers:\n",
    "    df = pd.read_csv(f\"../data/finance_data/{t}.csv\")\n",
    "    stock_data[t] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5b8911bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for missing values in each dataset:\n",
      "\n",
      "Summary statistics for AAPL:\n",
      "+----------+------------------+\n",
      "| Column   |   Missing Values |\n",
      "|----------+------------------|\n",
      "| Date     |                0 |\n",
      "| Close    |                0 |\n",
      "| High     |                0 |\n",
      "| Low      |                0 |\n",
      "| Open     |                0 |\n",
      "| Volume   |                0 |\n",
      "+----------+------------------+\n",
      "\n",
      "Summary statistics for AMZN:\n",
      "+----------+------------------+\n",
      "| Column   |   Missing Values |\n",
      "|----------+------------------|\n",
      "| Date     |                0 |\n",
      "| Close    |                0 |\n",
      "| High     |                0 |\n",
      "| Low      |                0 |\n",
      "| Open     |                0 |\n",
      "| Volume   |                0 |\n",
      "+----------+------------------+\n",
      "\n",
      "Summary statistics for GOOG:\n",
      "+----------+------------------+\n",
      "| Column   |   Missing Values |\n",
      "|----------+------------------|\n",
      "| Date     |                0 |\n",
      "| Close    |                0 |\n",
      "| High     |                0 |\n",
      "| Low      |                0 |\n",
      "| Open     |                0 |\n",
      "| Volume   |                0 |\n",
      "+----------+------------------+\n",
      "\n",
      "Summary statistics for META:\n",
      "+----------+------------------+\n",
      "| Column   |   Missing Values |\n",
      "|----------+------------------|\n",
      "| Date     |                0 |\n",
      "| Close    |                0 |\n",
      "| High     |                0 |\n",
      "| Low      |                0 |\n",
      "| Open     |                0 |\n",
      "| Volume   |                0 |\n",
      "+----------+------------------+\n",
      "\n",
      "Summary statistics for MSFT:\n",
      "+----------+------------------+\n",
      "| Column   |   Missing Values |\n",
      "|----------+------------------|\n",
      "| Date     |                0 |\n",
      "| Close    |                0 |\n",
      "| High     |                0 |\n",
      "| Low      |                0 |\n",
      "| Open     |                0 |\n",
      "| Volume   |                0 |\n",
      "+----------+------------------+\n",
      "\n",
      "Summary statistics for NVDA:\n",
      "+----------+------------------+\n",
      "| Column   |   Missing Values |\n",
      "|----------+------------------|\n",
      "| Date     |                0 |\n",
      "| Close    |                0 |\n",
      "| High     |                0 |\n",
      "| Low      |                0 |\n",
      "| Open     |                0 |\n",
      "| Volume   |                0 |\n",
      "+----------+------------------+\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking for missing values in each dataset:\")\n",
    "for ticker, df in stock_data.items():\n",
    "    print(f\"\\nSummary statistics for {ticker}:\")\n",
    "    missing_summary = df.isna().sum().reset_index()\n",
    "    missing_summary.columns = ['Column', 'Missing Values']\n",
    "    print(tabulate(missing_summary, headers=\"keys\", tablefmt=\"psql\", showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1567953e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and aligning datasets:\n",
      "+----------+---------------+--------------+----------------------+----------------+\n",
      "| Ticker   |   Rows Before |   Rows After |   Duplicates Removed | Date dtype     |\n",
      "|----------+---------------+--------------+----------------------+----------------|\n",
      "| AAPL     |          3774 |         3774 |                    0 | datetime64[ns] |\n",
      "| AMZN     |          3774 |         3774 |                    0 | datetime64[ns] |\n",
      "| GOOG     |          3774 |         3774 |                    0 | datetime64[ns] |\n",
      "| META     |          2923 |         2923 |                    0 | datetime64[ns] |\n",
      "| MSFT     |          3774 |         3774 |                    0 | datetime64[ns] |\n",
      "| NVDA     |          3774 |         3774 |                    0 | datetime64[ns] |\n",
      "+----------+---------------+--------------+----------------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning and aligning datasets:\")\n",
    "\n",
    "summary = []\n",
    "\n",
    "date_sets = [set(pd.to_datetime(df[\"Date\"])) for df in stock_data.values()]\n",
    "common_dates = set.intersection(*date_sets)\n",
    "\n",
    "for ticker, df in stock_data.items():\n",
    "    before_rows = df.shape[0]\n",
    "\n",
    "    # Convert Date column to datetime\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(subset=\"Date\", inplace=True)\n",
    "\n",
    "    # Filter to common dates\n",
    "    df = df[df[\"Date\"].isin(common_dates)].copy()\n",
    "\n",
    "    # Sort by date\n",
    "    df.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "    # Adjust index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Get after rows\n",
    "    after_rows = df.shape[0]\n",
    "\n",
    "    # Save back to dictionary\n",
    "    stock_data[ticker] = df\n",
    "\n",
    "    summary.append(\n",
    "        {\n",
    "            \"Ticker\": ticker,\n",
    "            \"Rows Before\": before_rows,\n",
    "            \"Rows After\": after_rows,\n",
    "            \"Duplicates Removed\": before_rows - after_rows,\n",
    "            \"Date dtype\": df[\"Date\"].dtype,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(tabulate(summary, headers=\"keys\", tablefmt=\"psql\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
